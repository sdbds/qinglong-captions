import argparse
import numpy as np
import time
from PIL import Image
from pathlib import Path
import lance
import pyarrow as pa
from rich.console import Console
from rich.pretty import Pretty
from rich.progress import (
    Progress,
    SpinnerColumn,
    TextColumn,
    BarColumn,
    TaskProgressColumn,
    TimeRemainingColumn,
    TimeElapsedColumn,
    TransferSpeedColumn,
    MofNCompleteColumn,
)
import torch
import onnxruntime as ort
from huggingface_hub import hf_hub_download
from module.lanceImport import transform2lance
import concurrent.futures
from transformers import AutoImageProcessor
import shutil

console = Console()

FILES = ["model.onnx"]


def preprocess_image(image):
    """ä½¿ç”¨processoré¢„å¤„ç†å›¾åƒ"""
    try:
        # è½¬æ¢ä¸ºRGBæ¨¡å¼
        if isinstance(image, np.ndarray):
            image = Image.fromarray(image).convert("RGB")
        elif isinstance(image, str) or isinstance(image, Path):
            image = Image.open(image).convert("RGB")
        elif not isinstance(image, Image.Image):
            raise TypeError("Input must be a PIL image, numpy array, or file path")

        # ä½¿ç”¨processoré¢„å¤„ç†å›¾åƒ
        inputs = processor(images=image, return_tensors="pt")

        # è½¬æ¢ä¸ºnumpyæ•°ç»„è¿”å›ž
        return inputs["pixel_values"][0].numpy()
    except Exception as e:
        console.print(f"[red]preprocess_image error: {str(e)}[/red]")
        return None


def load_and_preprocess_batch(uris):
    """å¹¶è¡ŒåŠ è½½å’Œé¢„å¤„ç†ä¸€æ‰¹å›¾åƒ"""

    def load_single_image(uri):
        try:
            # ç›´æŽ¥ä¼ å…¥è·¯å¾„ï¼Œåœ¨preprocess_imageä¸­å¤„ç†è½¬æ¢
            return preprocess_image(uri)
        except Exception as e:
            console.print(f"[red]Error processing {uri}: {str(e)}[/red]")
            return None

    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
        batch_images = list(executor.map(load_single_image, uris))

    # è¿‡æ»¤æŽ‰åŠ è½½å¤±è´¥çš„å›¾åƒ
    valid_images = [(i, img) for i, img in enumerate(batch_images) if img is not None]
    images = [img for _, img in valid_images]

    return images


def process_batch(images, session, input_name):
    """å¤„ç†å›¾åƒæ‰¹æ¬¡"""
    try:
        # å›¾åƒé€šè¿‡processorå¤„ç†åŽçš„numpyæ•°ç»„ï¼Œç›´æŽ¥å †å 
        batch_data = np.ascontiguousarray(np.stack(images))
        # æ‰§è¡ŒæŽ¨ç†
        outputs = session.run(None, {input_name: batch_data})
        return outputs[0]
    except Exception as e:
        console.print(f"[red]Batch processing error: {str(e)}[/red]")
        return None


def load_model(args):
    """åŠ è½½æ¨¡åž‹å’Œæ ‡ç­¾"""
    model_path = Path(args.model_dir) / args.repo_id.replace("/", "_") / "model.onnx"

    global processor
    processor = AutoImageProcessor.from_pretrained(args.repo_id, use_fast=True)

    # ä¸‹è½½æ¨¡åž‹
    if not model_path.exists() or args.force_download:
        for file in FILES:
            file_path = Path(args.model_dir) / args.repo_id.replace("/", "_") / file
            if not file_path.exists() or args.force_download:
                file_path = Path(
                    hf_hub_download(
                        repo_id=args.repo_id,
                        filename=file,
                        local_dir=file_path.parent,
                        force_download=args.force_download,
                    )
                )
                console.print(f"[blue]Downloaded {file} to {file_path}[/blue]")
            else:
                console.print(f"[green]Using existing {file}[/green]")

    # è®¾ç½®æŽ¨ç†æä¾›è€…
    providers = []
    if "TensorrtExecutionProvider" in ort.get_available_providers():
        providers.append("TensorrtExecutionProvider")
        console.print("[green]Using TensorRT for inference[/green]")
        console.print("[yellow]compile may take a long time, please wait...[/yellow]")
    elif "CUDAExecutionProvider" in ort.get_available_providers():
        providers.append("CUDAExecutionProvider")
        console.print("[green]Using CUDA for inference[/green]")
    elif "ROCMExecutionProvider" in ort.get_available_providers():
        providers.append("ROCMExecutionProvider")
        console.print("[green]Using ROCm for inference[/green]")
    elif "OpenVINOExecutionProvider" in ort.get_available_providers():
        providers = [("OpenVINOExecutionProvider", {"device_type": "GPU_FP32"})]
        console.print("[green]Using OpenVINO for inference[/green]")
    else:
        providers.append("CPUExecutionProvider")
        console.print("[yellow]Using CPU for inference[/yellow]")

    # åˆ›å»ºæŽ¨ç†ä¼šè¯
    sess_options = ort.SessionOptions()
    sess_options.graph_optimization_level = (
        ort.GraphOptimizationLevel.ORT_ENABLE_ALL
    )  # å¯ç”¨æ‰€æœ‰ä¼˜åŒ–

    if "CPUExecutionProvider" in providers:
        # CPUæ—¶å¯ç”¨å¤šçº¿ç¨‹æŽ¨ç†
        sess_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL  # å¯ç”¨å¹¶è¡Œæ‰§è¡Œ
        sess_options.inter_op_num_threads = 8  # è®¾ç½®çº¿ç¨‹æ•°
        sess_options.intra_op_num_threads = 8  # è®¾ç½®ç®—å­å†…éƒ¨å¹¶è¡Œæ•°

    # TensorRT ä¼˜åŒ–
    if "TensorrtExecutionProvider" in providers:
        sess_options.enable_mem_pattern = True
        sess_options.enable_mem_reuse = True
        providers_with_options = [
            (
                "TensorrtExecutionProvider",
                {
                    "trt_fp16_enable": True,  # Enable FP16 precision for faster inference
                    "trt_builder_optimization_level": 3,
                    "trt_max_partition_iterations": 1000,
                    "trt_engine_cache_enable": True,
                    "trt_engine_cache_path": f"{Path(args.model_dir) / args.repo_id.replace('/', '_')}/trt_engines",
                    "trt_engine_hw_compatible": True,
                    "trt_force_sequential_engine_build": False,
                    "trt_context_memory_sharing_enable": True,
                    "trt_timing_cache_enable": True,
                    "trt_timing_cache_path": f"{Path(args.model_dir) / args.repo_id.replace('/', '_')}",
                    "trt_sparsity_enable": True,
                    "trt_min_subgraph_size": 7,
                    # "trt_detailed_build_log": True,
                },
            ),
            (
                "CUDAExecutionProvider",
                {
                    "arena_extend_strategy": "kSameAsRequested",
                    "cudnn_conv_algo_search": "EXHAUSTIVE",
                    "do_copy_in_default_stream": True,
                    "cudnn_conv_use_max_workspace": "1",  # ä½¿ç”¨æœ€å¤§å·¥ä½œç©ºé—´
                    "tunable_op_enable": True,  # å¯ç”¨å¯è°ƒä¼˜æ“ä½œ
                    "tunable_op_tuning_enable": True,  # å¯ç”¨è°ƒä¼˜
                },
            ),
        ]

    elif "CUDAExecutionProvider" in providers:
        # CUDA GPU ä¼˜åŒ–
        sess_options.enable_mem_pattern = True
        sess_options.enable_mem_reuse = True
        providers_with_options = [
            (
                "CUDAExecutionProvider",
                {
                    "arena_extend_strategy": "kSameAsRequested",
                    "cudnn_conv_algo_search": "EXHAUSTIVE",
                    "do_copy_in_default_stream": True,
                    "cudnn_conv_use_max_workspace": "1",
                    "tunable_op_enable": True,
                    "tunable_op_tuning_enable": True,
                },
            ),
        ]
    else:
        providers_with_options = providers

    console.print(f"[cyan]Providers with options:[/cyan]")
    console.print(Pretty(providers_with_options, indent_guides=True, expand_all=True))
    start_time = time.time()
    ort_sess = ort.InferenceSession(
        str(model_path), sess_options=sess_options, providers=providers_with_options
    )
    input_name = ort_sess.get_inputs()[0].name
    console.print(
        f"[green]Model loaded in {time.time() - start_time:.2f} seconds[/green]"
    )
    return ort_sess, input_name


def main(args):
    global console

    watermark_dir = Path(args.train_data_dir) / "watermarked"
    no_watermark_dir = Path(args.train_data_dir) / "no_watermark"
    # ç¡®ä¿ç›®æ ‡æ–‡ä»¶å¤¹å­˜åœ¨
    if watermark_dir.exists():
        for symlink in watermark_dir.rglob("*"):
            if symlink.is_symlink():
                symlink.unlink()
    if no_watermark_dir.exists():
        for symlink in no_watermark_dir.rglob("*"):
            if symlink.is_symlink():
                symlink.unlink()

    # åˆå§‹åŒ– Lance æ•°æ®é›†
    if not isinstance(args.train_data_dir, lance.LanceDataset):
        if args.train_data_dir.endswith(".lance"):
            dataset = lance.dataset(args.train_data_dir)
        elif any(
            file.suffix == ".lance" for file in Path(args.train_data_dir).glob("*")
        ):
            lance_file = next(
                file
                for file in Path(args.train_data_dir).glob("*")
                if file.suffix == ".lance"
            )
            dataset = lance.dataset(str(lance_file))
        else:
            console.print("[yellow]Converting dataset to Lance format...[/yellow]")
            dataset = transform2lance(
                args.train_data_dir,
                output_name="dataset",
                save_binary=False,
                not_save_disk=False,
                tag="WatermarkDetection",
            )
            console.print("[green]Dataset converted to Lance format[/green]")

    else:
        dataset = args.train_data_dir
        console.print("[green]Using existing Lance dataset[/green]")

    ort_sess, input_name = load_model(args)

    # å…ˆè®¡ç®—å›¾ç‰‡æ€»æ•°
    total_images = len(
        dataset.to_table(
            columns=["mime"],
            filter=("mime LIKE 'image/%'"),
        )
    )

    # ç„¶åŽåˆ›å»ºå¸¦columnsçš„scannerå¤„ç†æ•°æ®
    scanner = dataset.scanner(
        columns=["uris", "mime", "captions"],
        filter=("mime LIKE 'image/%'"),
        scan_in_order=True,
        batch_size=args.batch_size,
        batch_readahead=16,
        fragment_readahead=4,
        io_buffer_size=32 * 1024 * 1024,  # 32MB buffer
        late_materialization=True,
    )

    with Progress(
        "[progress.description]{task.description}",
        SpinnerColumn(spinner_name="dots"),
        MofNCompleteColumn(separator="/"),
        BarColumn(bar_width=40, complete_style="green", finished_style="bold green"),
        TextColumn("â€¢"),
        TaskProgressColumn(),
        TextColumn("â€¢"),
        TransferSpeedColumn(),
        TextColumn("â€¢"),
        TimeElapsedColumn(),
        TextColumn("â€¢"),
        TimeRemainingColumn(),
        expand=True,
    ) as progress:
        task = progress.add_task("[bold cyan]Processing images...", total=total_images)

        console = progress.console

        # ç”¨äºŽæ”¶é›†ç»“æžœçš„åˆ—è¡¨
        detection_results = []

        for batch in scanner.to_batches():
            uris = batch["uris"].to_pylist()  # èŽ·å–æ–‡ä»¶è·¯å¾„

            # ä½¿ç”¨å¹¶è¡Œå¤„ç†åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
            batch_images = load_and_preprocess_batch(uris)

            if not batch_images:
                progress.update(task, advance=len(uris))
                continue

            # å¤„ç†æ‰¹æ¬¡
            probs = process_batch(batch_images, ort_sess, input_name)
            # åˆ›å»ºå¯¹åº”çš„ç›®æ ‡æ–‡ä»¶å¤¹ï¼ˆå¦‚æžœä¸å­˜åœ¨ï¼‰
            if probs is not None:
                for path, prob in zip(uris, probs):
                    # èŽ·å–æ°´å°æ£€æµ‹ç»“æžœ
                    watermark_prob = prob[1]  # ç´¢å¼•1å¯¹åº”"Watermark"æ ‡ç­¾

                    # æ ¹æ®æ¦‚çŽ‡ç¡®å®šæ˜¯å¦æœ‰æ°´å°ï¼ˆé˜ˆå€¼å¯ä»¥è°ƒæ•´ï¼‰
                    has_watermark = watermark_prob > args.thresh

                    # æ·»åŠ åˆ°ç»“æžœåˆ—è¡¨
                    detection_results.append((path, float(watermark_prob)))

                    # åˆ›å»ºè½¯é“¾æŽ¥
                    source_path = Path(path).absolute()
                    relative_path = source_path.relative_to(
                        Path(args.train_data_dir).absolute()
                    )
                    target_dir = watermark_dir if has_watermark else no_watermark_dir
                    target_path = target_dir / relative_path
                    target_path.parent.mkdir(parents=True, exist_ok=True)

                    # åˆ›å»ºè½¯é“¾æŽ¥
                    try:
                        target_path.symlink_to(source_path)
                    except (FileExistsError, PermissionError) as e:
                        console.print(
                            f"[red]Unable to create symlink for {path}: {e}[/red]"
                        )
                        # å¦‚æžœæ— æ³•åˆ›å»ºè½¯é“¾æŽ¥ï¼Œå°è¯•å¤åˆ¶æ–‡ä»¶ä»£æ›¿
                        try:
                            shutil.copy2(source_path, target_path)
                            console.print(
                                f"[yellow]Created copy instead of symlink for {path}[/yellow]"
                            )
                        except Exception as copy_err:
                            console.print(f"[red]Failed to copy file: {copy_err}[/red]")

            progress.update(task, advance=len(batch["uris"].to_pylist()))

    # ç»Ÿè®¡æ°´å°å›¾ç‰‡æ•°é‡
    watermark_count = sum(1 for _, prob in detection_results if prob > args.thresh)
    total_count = len(detection_results)

    # æŒ‰è·¯å¾„å±‚æ¬¡ç»“æž„ç»„ç»‡ç»“æžœ
    path_tree = {}
    for path, prob in detection_results:
        parts = Path(path).relative_to(Path(args.train_data_dir).absolute()).parts
        current = path_tree
        for i, part in enumerate(parts):
            if i == len(parts) - 1:
                # æœ€åŽä¸€å±‚æ˜¯æ–‡ä»¶åï¼Œå­˜å‚¨æ¦‚çŽ‡
                current[part] = f"{prob:.4f}" + (
                    "ðŸ”´(Watermarked)ðŸ”–" if prob > args.thresh else "ðŸŸ¢(No Watermark)ðŸ“„"
                )
            else:
                if part not in current:
                    current[part] = {}
                current = current[part]

    # ä½¿ç”¨Prettyæ‰“å°ç»“æžœæ ‘
    console.print("\n[bold green]Resultsï¼š[/bold green]")
    console.print(Pretty(path_tree, indent_guides=True, expand_all=True))
    # æ‰“å°æ£€æµ‹ç»“æžœç»Ÿè®¡
    console.print(
        f"ðŸ”´WatermarkedðŸ”–: {watermark_count} ({watermark_count/total_count*100:.2f}%)"
    )
    console.print(
        f"ðŸŸ¢No WatermarkðŸ“„: {total_count - watermark_count} ({(total_count - watermark_count)/total_count*100:.2f}%)"
    )


def setup_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "train_data_dir",
        type=str,
        help="Directory containing images to process",
    )
    parser.add_argument(
        "--repo_id",
        type=str,
        default="bdsqlsz/joycaption-watermark-detection-onnx",
        help="Repository ID for Watermark Detection model on Hugging Face",
    )
    parser.add_argument(
        "--model_dir",
        type=str,
        default="watermark_detection",
        help="Directory to store Watermark Detection model",
    )
    parser.add_argument(
        "--force_download",
        action="store_true",
        help="Force downloading Watermark Detection model",
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=16,
        help="Batch size for inference",
    )
    parser.add_argument(
        "--thresh",
        type=float,
        default=1.0,
        help="Default threshold for tag confidence",
    )

    return parser


if __name__ == "__main__":
    parser = setup_parser()

    args = parser.parse_args()

    main(args)
